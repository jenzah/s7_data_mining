{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP - Prétraitement des données - Achats alimentaires\n",
    "\n",
    "**Objectif**: Nettoyer et préparer le jeu de données bruité `donnees_brutes_achats.xlsx` pour le rendre exploitable.\n",
    "\n",
    "**Étapes**:\n",
    "1. Importer les données\n",
    "2. Diagnostiquer la qualité des données\n",
    "3. Gérer les doublons\n",
    "4. Uniformiser les chaînes de caractères\n",
    "5. Harmoniser les catégories\n",
    "6. Unifier le format des dates\n",
    "7. Gérer les valeurs manquantes\n",
    "8. Détecter et traiter les valeurs aberrantes\n",
    "9. Supprimer la colonne 'Notes'\n",
    "10. Exporter la table finale propre\n",
    "\n",
    "\n",
    "# **Rapport de nettoyage**\n",
    "\n",
    "### Problèmes détectés:\n",
    "1. **Doublons**: Présence de lignes dupliquées (exactes et par TransactionID)\n",
    "2. **Chaînes non uniformisées**: Espaces parasites, casse incohérente, synonymes\n",
    "3. **Catégories incohérentes**: Variations orthographiques et formats différents\n",
    "4. **Dates hétérogènes**: Formats de dates variés\n",
    "5. **Valeurs manquantes**: Dans les colonnes Quantité et Prix\n",
    "6. **Valeurs aberrantes**: Quantités négatives/excessives, prix à 999\n",
    "7. **Produits invalides**: Lignes avec '—' ou vides\n",
    "\n",
    "### Choix de nettoyage:\n",
    "- **Doublons**: Suppression des doublons exacts puis par TransactionID (conservation de la première occurrence)\n",
    "- **Uniformisation**: Trim des espaces, conversion en minuscules, normalisation des accents\n",
    "- **Synonymes**: Mapping manuel des variantes (pates→pâtes, tomato→tomate, etc.)\n",
    "- **Catégories**: Harmonisation via mapping (epicerie→épicerie, fruits-legumes→fruits & légumes)\n",
    "- **Dates**: Conversion avec `pd.to_datetime(dayfirst=True)` au format YYYY-MM-DD\n",
    "- **Valeurs manquantes**: Imputation par la **médiane** (plus robuste que la moyenne face aux outliers)\n",
    "- **Aberrants**: \n",
    "  - Quantités < 0 ou > 100: remplacement par la médiane\n",
    "  - Prix > 100: remplacement par la médiane\n",
    "  - Produits invalides: suppression des lignes\n",
    "- **Colonne Notes**: Supprimée (non utilisée pour l'analyse)\n",
    "\n",
    "### Impact:\n",
    "Les statistiques détaillées (nombre de lignes supprimées, valeurs modifiées) sont affichées dans les cellules ci-dessus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Nettoyage des données**\n",
    "## 1. Import des bibliothèques et des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre total de lignes: 51\n",
      "\n",
      "Premières lignes du dataset:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TransactionID</th>\n",
       "      <th>Produit</th>\n",
       "      <th>Quantité</th>\n",
       "      <th>Prix</th>\n",
       "      <th>Catégorie</th>\n",
       "      <th>Date</th>\n",
       "      <th>Notes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Pain</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.20</td>\n",
       "      <td>Boulangerie</td>\n",
       "      <td>2025-09-01</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Lait</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.95</td>\n",
       "      <td>Laitage</td>\n",
       "      <td>01/09/2025</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Beurre</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.80</td>\n",
       "      <td>Laitage</td>\n",
       "      <td>2025/09/01</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Tomate</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.99</td>\n",
       "      <td>Fruits &amp; Légumes</td>\n",
       "      <td>2025-09-02</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>tomato</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.10</td>\n",
       "      <td>fruits et legumes</td>\n",
       "      <td>02-09-2025</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>Pâtes</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.89</td>\n",
       "      <td>Épicerie</td>\n",
       "      <td>2025-09-02</td>\n",
       "      <td>promo? 2 pour 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>pates</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.89</td>\n",
       "      <td>epicerie</td>\n",
       "      <td>02/09/25</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>Riz</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.10</td>\n",
       "      <td>Épicerie</td>\n",
       "      <td>2025-09-03</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>Riz</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.10</td>\n",
       "      <td>épicerie</td>\n",
       "      <td>2025-09-03</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>Yaourt</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.45</td>\n",
       "      <td>Laitage</td>\n",
       "      <td>2025-09-03</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   TransactionID Produit  Quantité  Prix          Catégorie        Date  \\\n",
       "0              1    Pain       1.0  1.20        Boulangerie  2025-09-01   \n",
       "1              2    Lait       2.0  0.95            Laitage  01/09/2025   \n",
       "2              3  Beurre       1.0  2.80            Laitage  2025/09/01   \n",
       "3              4  Tomate       3.0  1.99   Fruits & Légumes  2025-09-02   \n",
       "4              5  tomato       2.0  2.10  fruits et legumes  02-09-2025   \n",
       "5              6   Pâtes       1.0  0.89           Épicerie  2025-09-02   \n",
       "6              7   pates       2.0  0.89           epicerie    02/09/25   \n",
       "7              8     Riz       1.0  1.10           Épicerie  2025-09-03   \n",
       "8              9    Riz        5.0  1.10           épicerie  2025-09-03   \n",
       "9             10  Yaourt       6.0  0.45            Laitage  2025-09-03   \n",
       "\n",
       "             Notes  \n",
       "0              NaN  \n",
       "1              NaN  \n",
       "2              NaN  \n",
       "3              NaN  \n",
       "4              NaN  \n",
       "5  promo? 2 pour 1  \n",
       "6              NaN  \n",
       "7              NaN  \n",
       "8              NaN  \n",
       "9              NaN  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Import data\n",
    "df = pd.read_excel('donnees_brutes_achats.xlsx')\n",
    "\n",
    "# Preview of dataset\n",
    "print(f\"Nombre total de lignes: {len(df)}\")\n",
    "print(\"\\nPremières lignes du dataset:\")\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Diagnostic de la qualité des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Informations sur le dataset ===\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 51 entries, 0 to 50\n",
      "Data columns (total 7 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   TransactionID  51 non-null     int64  \n",
      " 1   Produit        51 non-null     object \n",
      " 2   Quantité       49 non-null     float64\n",
      " 3   Prix           49 non-null     float64\n",
      " 4   Catégorie      49 non-null     object \n",
      " 5   Date           51 non-null     object \n",
      " 6   Notes          4 non-null      object \n",
      "dtypes: float64(2), int64(1), object(4)\n",
      "memory usage: 2.9+ KB\n"
     ]
    }
   ],
   "source": [
    "# General information\n",
    "print(\"=== Informations sur le dataset ===\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Description statistique ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TransactionID</th>\n",
       "      <th>Produit</th>\n",
       "      <th>Quantité</th>\n",
       "      <th>Prix</th>\n",
       "      <th>Catégorie</th>\n",
       "      <th>Date</th>\n",
       "      <th>Notes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>51.000000</td>\n",
       "      <td>51</td>\n",
       "      <td>49.000000</td>\n",
       "      <td>49.000000</td>\n",
       "      <td>49</td>\n",
       "      <td>51</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>NaN</td>\n",
       "      <td>40</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22</td>\n",
       "      <td>29</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Beurre</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Laitage</td>\n",
       "      <td>2025-09-06</td>\n",
       "      <td>promo? 2 pour 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>25.803922</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23.061224</td>\n",
       "      <td>22.349184</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>14.593176</td>\n",
       "      <td>NaN</td>\n",
       "      <td>142.494943</td>\n",
       "      <td>142.437880</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>13.500000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>26.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.750000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>38.500000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.800000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>50.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>999.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        TransactionID Produit     Quantité        Prix Catégorie        Date  \\\n",
       "count       51.000000      51    49.000000   49.000000        49          51   \n",
       "unique            NaN      40          NaN         NaN        22          29   \n",
       "top               NaN  Beurre          NaN         NaN   Laitage  2025-09-06   \n",
       "freq              NaN       4          NaN         NaN         9           4   \n",
       "mean        25.803922     NaN    23.061224   22.349184       NaN         NaN   \n",
       "std         14.593176     NaN   142.494943  142.437880       NaN         NaN   \n",
       "min          1.000000     NaN    -1.000000    0.000000       NaN         NaN   \n",
       "25%         13.500000     NaN     1.000000    0.900000       NaN         NaN   \n",
       "50%         26.000000     NaN     2.000000    1.750000       NaN         NaN   \n",
       "75%         38.500000     NaN     4.000000    2.800000       NaN         NaN   \n",
       "max         50.000000     NaN  1000.000000  999.000000       NaN         NaN   \n",
       "\n",
       "                  Notes  \n",
       "count                 4  \n",
       "unique                4  \n",
       "top     promo? 2 pour 1  \n",
       "freq                  1  \n",
       "mean                NaN  \n",
       "std                 NaN  \n",
       "min                 NaN  \n",
       "25%                 NaN  \n",
       "50%                 NaN  \n",
       "75%                 NaN  \n",
       "max                 NaN  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Statistical description\n",
    "print(\"\\n=== Description statistique ===\")\n",
    "df.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Valeurs manquantes ===\n",
      "           Nombre de valeurs manquantes  Pourcentage\n",
      "Quantité                              2     3.921569\n",
      "Prix                                  2     3.921569\n",
      "Catégorie                             2     3.921569\n",
      "Notes                                47    92.156863\n"
     ]
    }
   ],
   "source": [
    "# Missing values\n",
    "print(\"\\n=== Valeurs manquantes ===\")\n",
    "missing_values = df.isnull().sum()\n",
    "missing_percent = (df.isnull().sum() / len(df)) * 100\n",
    "missing_df = pd.DataFrame({\n",
    "    'Nombre de valeurs manquantes': missing_values,\n",
    "    'Pourcentage': missing_percent\n",
    "})\n",
    "print(missing_df[missing_df['Nombre de valeurs manquantes'] > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Analyse des doublons ===\n",
      "Nombre de doublons exacts: 1\n",
      "Nombre de doublons par TransactionID: 1\n"
     ]
    }
   ],
   "source": [
    "# Duplicates\n",
    "print(\"\\n=== Analyse des doublons ===\")\n",
    "nb_duplicates = df.duplicated().sum()\n",
    "print(f\"Nombre de doublons exacts: {nb_duplicates}\")\n",
    "\n",
    "# TransactionID duplicates\n",
    "nb_id_duplicates = df.duplicated(subset=['TransactionID']).sum()\n",
    "print(f\"Nombre de doublons par TransactionID: {nb_id_duplicates}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Valeurs uniques ===\n",
      "TransactionID: 50 valeurs uniques\n",
      "Produit: 40 valeurs uniques\n",
      "  Exemples: ['Pain' 'Lait' 'Beurre' 'Tomate' 'tomato' 'Pâtes' 'pates' 'Riz' 'Riz '\n",
      " 'Yaourt']\n",
      "Quantité: 12 valeurs uniques\n",
      "Prix: 34 valeurs uniques\n",
      "Catégorie: 22 valeurs uniques\n",
      "  Exemples: ['Boulangerie' 'Laitage' 'Fruits & Légumes' 'fruits et legumes' 'Épicerie'\n",
      " 'epicerie' 'épicerie' 'laitage' 'Œufs & Ovoproduits' 'oeufs']\n",
      "Date: 29 valeurs uniques\n",
      "Notes: 4 valeurs uniques\n"
     ]
    }
   ],
   "source": [
    "# Unique values\n",
    "print(\"\\n=== Valeurs uniques ===\")\n",
    "for col in df.columns:\n",
    "    print(f\"{col}: {df[col].nunique()} valeurs uniques\")\n",
    "    if col in ['Produit', 'Catégorie'] and df[col].nunique() < 50:\n",
    "        print(f\"  Exemples: {df[col].unique()[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Gestion des doublons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de lignes avant suppression des doublons: 51\n",
      "\n",
      "Nombre de lignes après suppression des doublons exacts: 50\n",
      "Doublons exacts supprimés: 1\n",
      "\n",
      "Nombre de lignes après suppression des doublons par TransactionID: 50\n",
      "Doublons par TransactionID supprimés: 0\n"
     ]
    }
   ],
   "source": [
    "# Save original df lenght\n",
    "len_original = len(df)\n",
    "print(f\"Nombre de lignes avant suppression des doublons: {len_original}\\n\")\n",
    "\n",
    "# Delete duplicates\n",
    "df.drop_duplicates(inplace=True)\n",
    "print(f\"Nombre de lignes après suppression des doublons exacts: {len(df)}\")\n",
    "print(f\"Doublons exacts supprimés: {len_original - len(df)}\\n\")\n",
    "\n",
    "# Delete ID duplicates (keep first)\n",
    "len_before_id = len(df)\n",
    "df.drop_duplicates(subset=['TransactionID'], inplace=True)\n",
    "print(f\"Nombre de lignes après suppression des doublons par TransactionID: {len(df)}\")\n",
    "print(f\"Doublons par TransactionID supprimés: {len_before_id - len(df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Uniformisation des chaînes de caractères"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Uniformisation des chaînes ===\n",
      "\n",
      "Exemples de produits avant nettoyage:\n",
      "['Pain', 'Lait', 'Beurre', 'Tomate', 'tomato', 'Pâtes', 'pates', 'Riz', 'Riz ', 'Yaourt', 'yaourts', 'Oeufs', 'oeuf', 'Poulet', 'Poisson', 'Banane', 'bananes', 'Pomme', 'Pommes', 'Concombre']\n"
     ]
    }
   ],
   "source": [
    "# Trim spaces, and convert into lower case\n",
    "print(\"=== Uniformisation des chaînes ===\")\n",
    "print(f\"\\nExemples de produits avant nettoyage:\")\n",
    "print(df['Produit'].head(20).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>column</th>\n",
       "      <th>affected_rows</th>\n",
       "      <th>percentage</th>\n",
       "      <th>examples_before</th>\n",
       "      <th>examples_after</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Produit</td>\n",
       "      <td>3</td>\n",
       "      <td>6.0</td>\n",
       "      <td>\"Riz \" | \"Cafe \" | \"Jambon \"</td>\n",
       "      <td>\"Riz\" | \"Cafe\" | \"Jambon\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Notes</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>\"   note à supprimer   \"</td>\n",
       "      <td>\"note à supprimer\"</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    column  affected_rows  percentage               examples_before  \\\n",
       "0  Produit              3         6.0  \"Riz \" | \"Cafe \" | \"Jambon \"   \n",
       "1    Notes              1         2.0      \"   note à supprimer   \"   \n",
       "\n",
       "              examples_after  \n",
       "0  \"Riz\" | \"Cafe\" | \"Jambon\"  \n",
       "1         \"note à supprimer\"  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Trimmed whitespace from 2 columns\n"
     ]
    }
   ],
   "source": [
    "def find_whitespace_in_values(df):\n",
    "    \"\"\"Find columns with leading/trailing whitespace in values\"\"\"\n",
    "    whitespace_info = []\n",
    "    string_cols = df.select_dtypes(include=['object', 'string']).columns\n",
    "    for col in string_cols:\n",
    "        has_whitespace = df[col].astype(str).str.strip() != df[col].astype(str)\n",
    "        if has_whitespace.any():\n",
    "            count = has_whitespace.sum()\n",
    "            whitespace_mask = has_whitespace\n",
    "            examples_original = df[col][whitespace_mask].head(3).tolist()\n",
    "            examples_cleaned = [str(val).strip() for val in examples_original]\n",
    "            examples_orig_str = ' | '.join([f'\"{val}\"' for val in examples_original])\n",
    "            examples_clean_str = ' | '.join([f'\"{val}\"' for val in examples_cleaned])\n",
    "            whitespace_info.append({\n",
    "                'column': col,\n",
    "                'affected_rows': count,\n",
    "                'percentage': round(count / len(df) * 100, 2),\n",
    "                'examples_before': examples_orig_str,\n",
    "                'examples_after': examples_clean_str\n",
    "            })\n",
    "    return pd.DataFrame(whitespace_info)\n",
    "\n",
    "def trim_whitespace(df, columns=None):\n",
    "    \"\"\"Trim whitespace from specified columns (or all string columns if None)\"\"\"\n",
    "    string_cols = df.select_dtypes(include=['object', 'string']).columns\n",
    "    if columns is not None:\n",
    "        string_cols = [col for col in columns if col in string_cols]\n",
    "    \n",
    "    for col in string_cols:\n",
    "        df[col] = df[col].str.strip()\n",
    "    \n",
    "    print(f\"✓ Trimmed whitespace from {len(string_cols)} columns\")\n",
    "    return df\n",
    "\n",
    "whitespace_issues = find_whitespace_in_values(df)\n",
    "display(whitespace_issues)\n",
    "\n",
    "if len(whitespace_issues) > 0:\n",
    "    whitespace_columns = whitespace_issues['column'].tolist()\n",
    "    df = trim_whitespace(df, whitespace_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardise strings - remove accents\n",
    "df['Produit'] = (df['Produit']\n",
    "    .str.lower()\n",
    "    .str.replace('é', 'e', regex=False)\n",
    "    .str.replace('è', 'e', regex=False)\n",
    "    .str.replace('ê', 'e', regex=False)\n",
    "    .str.replace('à', 'a', regex=False)\n",
    "    .str.replace('â', 'a', regex=False)\n",
    "    .str.replace('î', 'i', regex=False)\n",
    "    .str.replace('ï', 'i', regex=False)\n",
    "    .str.replace('ô', 'o', regex=False)\n",
    "    .str.replace('ö', 'o', regex=False)\n",
    "    .str.replace('û', 'u', regex=False)\n",
    "    .str.replace('ü', 'u', regex=False)\n",
    "    .str.replace('ç', 'c', regex=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Harmonisation des valeurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Standardised case in 1 columns\n"
     ]
    }
   ],
   "source": [
    "def find_case_insensitive_duplicates(df):\n",
    "    \"\"\"\n",
    "    Finds columns with case-insensitive duplicates (e.g., 'Apple', 'apple').\n",
    "    Returns a DataFrame summarizing the issues for easy display.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    string_cols = df.select_dtypes(include=['object', 'string']).columns\n",
    "    for col in string_cols:\n",
    "        series = df[col]\n",
    "        clean_series = series.dropna().astype(str)\n",
    "        if len(clean_series) == 0:\n",
    "            continue\n",
    "        case_map = {}\n",
    "        for value in clean_series.unique():\n",
    "            lower_val = value.lower()\n",
    "            if lower_val in case_map:\n",
    "                case_map[lower_val].append(value)\n",
    "            else:\n",
    "                case_map[lower_val] = [value]\n",
    "        duplicate_groups = [group for group in case_map.values() if len(group) > 1]\n",
    "        if duplicate_groups:\n",
    "            value_counts = df[col].value_counts()\n",
    "            total_affected_rows = 0\n",
    "            example_groups = []\n",
    "            for group in duplicate_groups:\n",
    "                total_affected_rows += value_counts[group].sum()\n",
    "                most_frequent_form = max(group, key=lambda x: value_counts.get(x, 0))\n",
    "                group_str = ' | '.join([f'\"{val}\"' for val in sorted(group)])\n",
    "                example_groups.append(f'{group_str} -> \"{most_frequent_form}\"')\n",
    "            results.append({\n",
    "                'column': col,\n",
    "                'duplicate_groups': len(duplicate_groups),\n",
    "                'affected_rows': total_affected_rows,\n",
    "                'examples': ' || '.join(example_groups[:3])\n",
    "            })\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "def standardise_case(df, columns: list):\n",
    "    \"\"\"\n",
    "    Standardises the casing of values in the specified columns.\n",
    "    \"\"\"\n",
    "    standardised_count = 0\n",
    "    for col in columns:\n",
    "        if col not in df.columns:\n",
    "            continue\n",
    "        series = df[col]\n",
    "        clean_series = series.dropna().astype(str)\n",
    "        if len(clean_series) == 0:\n",
    "            continue\n",
    "        case_map = {}\n",
    "        for value in clean_series.unique():\n",
    "            lower_val = value.lower()\n",
    "            if lower_val in case_map:\n",
    "                case_map[lower_val].append(value)\n",
    "            else:\n",
    "                case_map[lower_val] = [value]\n",
    "        duplicate_groups = [group for group in case_map.values() if len(group) > 1]\n",
    "        if not duplicate_groups:\n",
    "            continue\n",
    "        value_counts = df[col].value_counts()\n",
    "        replacement_map = {}\n",
    "        for group in duplicate_groups:\n",
    "            most_frequent_form = max(group, key=lambda x: value_counts.get(x, 0))\n",
    "            for variant in group:\n",
    "                if variant != most_frequent_form:\n",
    "                    replacement_map[variant] = most_frequent_form\n",
    "        if replacement_map:\n",
    "            df[col] = df[col].replace(replacement_map)\n",
    "            standardised_count += 1\n",
    "    \n",
    "    print(f\"✓ Standardised case in {standardised_count} columns\")\n",
    "    return df\n",
    "\n",
    "case_insensitive = find_case_insensitive_duplicates(df)\n",
    "if len(case_insensitive) > 0:\n",
    "    case_insensitive_columns = case_insensitive['column'].tolist()\n",
    "    df = standardise_case(df, case_insensitive_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Standardised case in 0 columns\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['Boulangerie', 'Laitage', 'Fruits & Légumes', 'fruits et legumes',\n",
       "       'Épicerie', 'epicerie', 'Œufs & Ovoproduits', 'oeufs', 'Boucherie',\n",
       "       'Poissonnerie', 'fruits-legumes', 'Fruits/Légumes', 'Boissons',\n",
       "       'boisson', nan, 'Crèmerie', 'Cremerie', 'Charcuterie', 'Divers'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = standardise_case(df, ['Catégorie'])\n",
    "df['Catégorie'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'column': 'Produit',\n",
       "  'fuzzy_groups': [['yaourt', 'yaourts'],\n",
       "   [\"huile d'olive\", 'huile olive'],\n",
       "   ['pomme', 'pommes'],\n",
       "   ['choco lait', 'chocolat'],\n",
       "   ['corn flakes', 'corn-flakes'],\n",
       "   ['coca cola', 'coca-cola'],\n",
       "   ['eau  minerale', 'eau minerale'],\n",
       "   ['fromage', 'frommage'],\n",
       "   ['oeuf', 'oeufs'],\n",
       "   ['banane', 'bananes']]},\n",
       " {'column': 'Catégorie',\n",
       "  'fuzzy_groups': [['Cremerie', 'Crèmerie'],\n",
       "   ['epicerie', 'Épicerie'],\n",
       "   ['Fruits & Légumes', 'Fruits/Légumes', 'fruits-legumes'],\n",
       "   ['Boissons', 'boisson'],\n",
       "   ['fruits et legumes', 'fruits-legumes']]}]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Applied 10 fuzzy mappings to column 'Produit'\n",
      "✓ Applied 5 fuzzy mappings to column 'Catégorie'\n"
     ]
    }
   ],
   "source": [
    "# Treat fuzzy duplicates\n",
    "import re\n",
    "from rapidfuzz import process, fuzz\n",
    "\n",
    "def normalise_for_comparison(s: str) -> str:\n",
    "    \"\"\"Intelligently cleans a string for a base similarity comparison.\"\"\"\n",
    "    if not isinstance(s, str):\n",
    "        return \"\"\n",
    "    s_lower = s.lower()\n",
    "    s_lower = re.sub(r'tbc\\s*\\(proposition\\s*-?|local\\s*|à\\s*confirmer|pp\\s*\\d', '', s_lower)\n",
    "    s_lower = re.sub(r'[\\s-]+', '', s_lower)\n",
    "    s_lower = s_lower.strip(\"()[]{}'\\\"- \")\n",
    "    return s_lower\n",
    "\n",
    "def find_fuzzy_duplicates(df, threshold: int = 85, min_length: int = 3):\n",
    "    \"\"\"\n",
    "    Finds groups of similar strings (potential typos) in categorical columns.\n",
    "    \"\"\"\n",
    "    issue_list = []\n",
    "    string_cols = df.select_dtypes(include=['object', 'string']).columns\n",
    "    for col in string_cols:\n",
    "        series = df[col]\n",
    "        if series.nunique() < 2 or series.nunique() > 2000:\n",
    "            continue\n",
    "        categories = series.dropna().unique().tolist()\n",
    "        filtered_cats = [\n",
    "            cat for cat in set(categories)\n",
    "            if isinstance(cat, str) and len(cat) >= min_length and not re.search(r'\\d', cat)\n",
    "        ]\n",
    "        if len(filtered_cats) < 2:\n",
    "            continue\n",
    "        normalised_cats = [normalise_for_comparison(cat) for cat in filtered_cats]\n",
    "        score_matrix = process.cdist(normalised_cats, normalised_cats, scorer=fuzz.ratio, score_cutoff=threshold)\n",
    "        groups = []\n",
    "        processed_indices = set()\n",
    "        for i in range(len(filtered_cats)):\n",
    "            if i in processed_indices:\n",
    "                continue\n",
    "            nonzero_result = score_matrix[i].nonzero()\n",
    "            if isinstance(nonzero_result, tuple) and len(nonzero_result) > 0:\n",
    "                similar_indices = nonzero_result[0] if len(nonzero_result) == 1 else nonzero_result[1]\n",
    "            else:\n",
    "                continue\n",
    "            if len(similar_indices) > 1:\n",
    "                current_group = {filtered_cats[j] for j in similar_indices}\n",
    "                groups.append(sorted(list(current_group)))\n",
    "                processed_indices.update(similar_indices)\n",
    "        if groups:\n",
    "            issue_list.append({'column': col, 'fuzzy_groups': groups})\n",
    "    return issue_list\n",
    "\n",
    "def standardise_fuzzy_values(df, column: str, mappings: dict):\n",
    "    \"\"\"\n",
    "    Standardises values in a column based on a provided mapping.\n",
    "    \"\"\"\n",
    "    if column in df.columns and mappings:\n",
    "        df[column] = df[column].replace(mappings)\n",
    "        print(f\"✓ Applied {len(mappings)} fuzzy mappings to column '{column}'\")\n",
    "    return df\n",
    "\n",
    "fuzzy_duplicates = find_fuzzy_duplicates(df)\n",
    "display(fuzzy_duplicates)\n",
    "\n",
    "for fuzzy_dict in fuzzy_duplicates:\n",
    "    column_name = fuzzy_dict['column']\n",
    "    fuzzy_groups = fuzzy_dict['fuzzy_groups']\n",
    "    \n",
    "    # Create mappings: all variants map to the first value in each group\n",
    "    mappings = {}\n",
    "    for group in fuzzy_groups:\n",
    "        if len(group) > 1:\n",
    "            canonical_value = group[0]  # First value is the standard\n",
    "            for variant in group[1:]:    # All other values are variants\n",
    "                mappings[variant] = canonical_value\n",
    "    \n",
    "    # Standardize the column using the mappings\n",
    "    df = standardise_fuzzy_values(df, column_name, mappings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Catégies uniques après harmonisation: ['Boulangerie' 'Laitage' 'Fruits & Légumes' 'Epicerie'\n",
      " 'Œufs & Ovoproduits' 'Boucherie' 'Poissonnerie' 'Boissons' nan 'Cremerie'\n",
      " 'Charcuterie' 'Divers']\n",
      "\n",
      "Produits uniques après harmonisation: ['pain' 'lait' 'beurre' 'tomate' 'pates' 'riz' 'yaourt' 'oeuf' 'poulet'\n",
      " 'poisson' 'banane' 'pomme' 'concombre' 'coca cola' 'eau  minerale' 'cafe'\n",
      " 'the' 'choco lait' 'fromage' 'jambon' '—' \"huile d'olive\" 'corn flakes'\n",
      " 'beurre demi-sel']\n"
     ]
    }
   ],
   "source": [
    "# Treat fuzzy duplicates\n",
    "mapping_products = {\n",
    "    'tomato': 'tomate',\n",
    "}\n",
    "\n",
    "mapping_category = {\n",
    "    'fruits et legumes': 'Fruits & Légumes',\n",
    "    'oeufs': 'Œufs & Ovoproduits',\n",
    "    'epicerie': 'Epicerie'\n",
    "}\n",
    "\n",
    "# # Apply mapping\n",
    "df['Produit'] = df['Produit'].replace(mapping_products)\n",
    "df['Catégorie'] = df['Catégorie'].replace(mapping_category)\n",
    "\n",
    "print(f\"\\nCatégies uniques après harmonisation: {df['Catégorie'].unique()}\")\n",
    "print(f\"\\nProduits uniques après harmonisation: {df['Produit'].unique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Unification du format des dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Unification des dates ===\n",
      "\n",
      "Exemples de dates avant conversion:\n",
      "0     2025-09-01\n",
      "1     01/09/2025\n",
      "2     2025/09/01\n",
      "3     2025-09-02\n",
      "4     02-09-2025\n",
      "5     2025-09-02\n",
      "6       02/09/25\n",
      "7     2025-09-03\n",
      "8     2025-09-03\n",
      "9     2025-09-03\n",
      "10    03/09/2025\n",
      "11    2025-09-04\n",
      "12    04-09-2025\n",
      "13    2025-09-04\n",
      "14    2025-09-04\n",
      "15    2025-09-05\n",
      "16    05/09/2025\n",
      "17    2025/09/05\n",
      "18    2025-09-05\n",
      "19    2025-09-06\n",
      "Name: Date, dtype: object\n",
      "Type de la colonne Date: object\n",
      "\n",
      "--- Conversion des dates en cours ---\n",
      "\n",
      "Exemples de dates après conversion:\n",
      "0    2025-09-01\n",
      "1    2025-01-09\n",
      "2    2025-09-01\n",
      "3    2025-09-02\n",
      "4    2025-02-09\n",
      "5    2025-09-02\n",
      "6    2025-02-09\n",
      "7    2025-09-03\n",
      "8    2025-09-03\n",
      "9    2025-09-03\n",
      "10   2025-03-09\n",
      "11   2025-09-04\n",
      "12   2025-04-09\n",
      "13   2025-09-04\n",
      "14   2025-09-04\n",
      "15   2025-09-05\n",
      "16   2025-05-09\n",
      "17   2025-09-05\n",
      "18   2025-09-05\n",
      "19   2025-09-06\n",
      "Name: Date, dtype: datetime64[ns]\n",
      "Type de la colonne Date: datetime64[ns]\n",
      "\n",
      "Nombre de dates invalides: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Unification des dates ===\")\n",
    "print(f\"\\nExemples de dates avant conversion:\")\n",
    "print(df['Date'].head(20))\n",
    "print(f\"Type de la colonne Date: {df['Date'].dtype}\")\n",
    "\n",
    "# Function to parse dates with multiple formats\n",
    "def parse_mixed_dates(date_str):\n",
    "    \"\"\"\n",
    "    Parse dates with mixed formats:\n",
    "    - YYYY-MM-DD, YYYY/MM/DD (year first)\n",
    "    - DD/MM/YYYY, DD-MM-YYYY, DD/MM/YY (day first)\n",
    "    \"\"\"\n",
    "    if pd.isna(date_str):\n",
    "        return pd.NaT\n",
    "    \n",
    "    date_str = str(date_str).strip()\n",
    "    \n",
    "    try:\n",
    "        return pd.to_datetime(date_str)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Otherwise try day-first format (DD/MM/YYYY, DD-MM-YYYY, DD/MM/YY)\n",
    "    return pd.to_datetime(date_str, dayfirst=True, errors='coerce')\n",
    "\n",
    "# Apply the parsing function\n",
    "print(\"\\n--- Conversion des dates en cours ---\")\n",
    "df['Date'] = df['Date'].apply(parse_mixed_dates)\n",
    "\n",
    "print(f\"\\nExemples de dates après conversion:\")\n",
    "print(df['Date'].head(20))\n",
    "print(f\"Type de la colonne Date: {df['Date'].dtype}\")\n",
    "\n",
    "# Check for unconverted dates (NaT)\n",
    "invalid_dates = df['Date'].isna().sum()\n",
    "print(f\"\\nNombre de dates invalides: {invalid_dates}\")\n",
    "\n",
    "if invalid_dates > 0:\n",
    "    print(\"\\nDates qui n'ont pas pu être converties:\")\n",
    "    print(df[df['Date'].isna()][['TransactionID', 'Produit', 'Date']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Gestion des valeurs manquantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Gestion des valeurs manquantes ===\n",
      "\n",
      "Valeurs manquantes avant traitement:\n",
      "Quantité    2\n",
      "Prix        2\n",
      "dtype: int64\n",
      "\n",
      "Statistiques Quantité avant imputation:\n",
      "count      48.000000\n",
      "mean       23.520833\n",
      "std       143.966159\n",
      "min        -1.000000\n",
      "25%         1.000000\n",
      "50%         2.000000\n",
      "75%         4.250000\n",
      "max      1000.000000\n",
      "Name: Quantité, dtype: float64\n",
      "\n",
      "Statistiques Prix avant imputation:\n",
      "count     48.000000\n",
      "mean      22.756458\n",
      "std      143.916366\n",
      "min        0.000000\n",
      "25%        0.897500\n",
      "50%        1.575000\n",
      "75%        2.825000\n",
      "max      999.000000\n",
      "Name: Prix, dtype: float64\n",
      "\n",
      "Valeur médiane pour Quantité: 2.0\n",
      "Valeur médiane pour Prix: 1.575\n",
      "\n",
      "Nombre de valeurs imputées:\n",
      "  - Quantité: 2\n",
      "  - Prix: 2\n",
      "\n",
      "Valeurs manquantes après traitement:\n",
      "Quantité    0\n",
      "Prix        0\n",
      "dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zahor\\AppData\\Local\\Temp\\ipykernel_26604\\2178208705.py:22: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['Quantité'].fillna(median_quantity, inplace=True)\n",
      "C:\\Users\\zahor\\AppData\\Local\\Temp\\ipykernel_26604\\2178208705.py:23: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['Prix'].fillna(median_price, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Gestion des valeurs manquantes ===\")\n",
    "print(f\"\\nValeurs manquantes avant traitement:\")\n",
    "print(df[['Quantité', 'Prix']].isnull().sum())\n",
    "\n",
    "# Statistics before imputation\n",
    "print(f\"\\nStatistiques Quantité avant imputation:\")\n",
    "print(df['Quantité'].describe())\n",
    "print(f\"\\nStatistiques Prix avant imputation:\")\n",
    "print(df['Prix'].describe())\n",
    "\n",
    "# Impute missing values (median)\n",
    "median_quantity = df['Quantité'].median()\n",
    "median_price = df['Prix'].median()\n",
    "\n",
    "print(f\"\\nValeur médiane pour Quantité: {median_quantity}\")\n",
    "print(f\"Valeur médiane pour Prix: {median_price}\")\n",
    "\n",
    "# Fill empties\n",
    "nb_missing_quantity = df['Quantité'].isnull().sum()\n",
    "nb_missing_price = df['Prix'].isnull().sum()\n",
    "\n",
    "df['Quantité'].fillna(median_quantity, inplace=True)\n",
    "df['Prix'].fillna(median_price, inplace=True)\n",
    "\n",
    "print(f\"\\nNombre de valeurs imputées:\")\n",
    "print(f\"  - Quantité: {nb_missing_quantity}\")\n",
    "print(f\"  - Prix: {nb_missing_price}\")\n",
    "\n",
    "print(f\"\\nValeurs manquantes après traitement:\")\n",
    "print(df[['Quantité', 'Prix']].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Gestion des valeurs manquantes ===\n",
      "\n",
      "Valeurs manquantes avant traitement:\n",
      "2\n",
      "\n",
      "Lignes avec catégorie manquante:\n",
      "   Produit Catégorie\n",
      "35  tomate       NaN\n",
      "49       —       NaN\n",
      "\n",
      "Mapping Produit -> Catégorie créé avec 24 produits\n",
      "  Rempli: tomate -> Fruits & Légumes\n",
      "  Rempli: — -> Divers\n",
      "\n",
      "Lignes supprimées (catégorie toujours manquante): 0\n",
      "\n",
      "Valeurs manquantes après traitement:\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Gestion des valeurs manquantes ===\")\n",
    "print(f\"\\nValeurs manquantes avant traitement:\")\n",
    "print(df['Catégorie'].isnull().sum())\n",
    "\n",
    "# Examine rows with missing categories\n",
    "missing_cat_mask = df['Catégorie'].isnull()\n",
    "print(f\"\\nLignes avec catégorie manquante:\")\n",
    "print(df[missing_cat_mask][['Produit', 'Catégorie']])\n",
    "\n",
    "product_category_map = (\n",
    "    df[df['Catégorie'].notna()]\n",
    "    .groupby('Produit')['Catégorie']\n",
    "    .agg(lambda x: x.mode()[0] if not x.mode().empty else None)\n",
    "    .to_dict())\n",
    "\n",
    "print(f\"\\nMapping Produit -> Catégorie créé avec {len(product_category_map)} produits\")\n",
    "\n",
    "# Fill missing categories based on product\n",
    "for idx in df[missing_cat_mask].index:\n",
    "    product = df.loc[idx, 'Produit']\n",
    "    if pd.notna(product) and product in product_category_map:\n",
    "        df.loc[idx, 'Catégorie'] = product_category_map[product]\n",
    "        print(f\"  Rempli: {product} -> {product_category_map[product]}\")\n",
    "\n",
    "# Remove rows that still have missing categories (no product or unknown product)\n",
    "rows_before = len(df)\n",
    "df = df[df['Catégorie'].notna()]\n",
    "rows_after = len(df)\n",
    "print(f\"\\nLignes supprimées (catégorie toujours manquante): {rows_before - rows_after}\")\n",
    "\n",
    "print(f\"\\nValeurs manquantes après traitement:\")\n",
    "print(df['Catégorie'].isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Détection et traitement des valeurs aberrantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Détection des valeurs aberrantes ===\n",
      "\n",
      "Quantités négatives: 1\n",
      "    TransactionID Produit  Quantité\n",
      "31             32    lait      -1.0\n",
      "\n",
      "Quantités excessives (>100): 1\n",
      "    TransactionID Produit  Quantité\n",
      "32             33    pain    1000.0\n",
      "\n",
      "Prix aberrants (>100): 1\n",
      "    TransactionID Produit   Prix\n",
      "33             34   pates  999.0\n",
      "\n",
      "Produits vides ou '—': 2\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Détection des valeurs aberrantes ===\")\n",
    "\n",
    "# Negative quantity\n",
    "negative_quantity = (df['Quantité'] < 0).sum()\n",
    "print(f\"\\nQuantités négatives: {negative_quantity}\")\n",
    "if negative_quantity > 0:\n",
    "    print(df[df['Quantité'] < 0][['TransactionID', 'Produit', 'Quantité']])\n",
    "\n",
    "# Quantity > 100\n",
    "excessive_quantity = (df['Quantité'] > 100).sum()\n",
    "print(f\"\\nQuantités excessives (>100): {excessive_quantity}\")\n",
    "if excessive_quantity > 0:\n",
    "    print(df[df['Quantité'] > 100][['TransactionID', 'Produit', 'Quantité']])\n",
    "\n",
    "# Outlier prices (> 100)\n",
    "outlier_prices = (df['Prix'] > 100).sum()\n",
    "print(f\"\\nPrix aberrants (>100): {outlier_prices}\")\n",
    "if outlier_prices > 0:\n",
    "    print(df[df['Prix'] > 100][['TransactionID', 'Produit', 'Prix']])\n",
    "\n",
    "# Empty products\n",
    "empty_products = (df['Produit'] == '—').sum() + (df['Produit'] == '').sum()\n",
    "print(f\"\\nProduits vides ou '—': {empty_products}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Traitement des valeurs aberrantes ===\n",
      "\n",
      "Nombre de lignes supprimées (produits invalides): 2\n",
      "Nombre total de lignes restantes: 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zahor\\AppData\\Local\\Temp\\ipykernel_26604\\2581040013.py:13: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['Quantité'].fillna(median_quantity, inplace=True)\n",
      "C:\\Users\\zahor\\AppData\\Local\\Temp\\ipykernel_26604\\2581040013.py:20: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['Prix'].fillna(median_price, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Traitement des valeurs aberrantes ===\")\n",
    "len_before_cleaning = len(df)\n",
    "\n",
    "# Negative quantity: convert into NaN, then fill with median\n",
    "df.loc[df['Quantité'] < 0, 'Quantité'] = pd.NA\n",
    "nb_corrected_neg_quantity = df['Quantité'].isna().sum()\n",
    "\n",
    "# Excessive quantity: convert into NaN, then fill with median\n",
    "df.loc[df['Quantité'] > 100, 'Quantité'] = pd.NA\n",
    "nb_corrected_exc_quantity = df['Quantité'].isna().sum() - nb_corrected_neg_quantity\n",
    "\n",
    "# Impute outlier quantities\n",
    "df['Quantité'].fillna(median_quantity, inplace=True)\n",
    "\n",
    "# Outlier prices\n",
    "df.loc[df['Prix'] > 100, 'Prix'] = pd.NA\n",
    "nb_corrected_price_outlier = df['Prix'].isna().sum()\n",
    "\n",
    "# Impute outlier prices\n",
    "df['Prix'].fillna(median_price, inplace=True)\n",
    "\n",
    "# Delete empties\n",
    "df = df[df['Produit'] != '—']\n",
    "df = df[df['Produit'] != '']\n",
    "df = df[df['Produit'] != 'nan']\n",
    "\n",
    "len_after_cleaning = len(df)\n",
    "nb_deleted = len_before_cleaning - len_after_cleaning\n",
    "\n",
    "print(f\"\\nNombre de lignes supprimées (produits invalides): {nb_deleted}\")\n",
    "print(f\"Nombre total de lignes restantes: {len_after_cleaning}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Suppression de la colonne 'Notes'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Suppression de la colonne Notes ===\n",
      "\n",
      "Colonnes avant suppression: ['TransactionID', 'Produit', 'Quantité', 'Prix', 'Catégorie', 'Date', 'Notes']\n",
      "Colonnes après suppression: ['TransactionID', 'Produit', 'Quantité', 'Prix', 'Catégorie', 'Date']\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Suppression de la colonne Notes ===\")\n",
    "print(f\"\\nColonnes avant suppression: {df.columns.tolist()}\")\n",
    "\n",
    "# Delete columns\n",
    "if 'Notes' in df.columns:\n",
    "    df = df.drop(columns=['Notes'])\n",
    "\n",
    "print(f\"Colonnes après suppression: {df.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Vérification finale et export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Vérification finale ===\n",
      "\n",
      "Dimensions du dataset final: (48, 6)\n",
      "\n",
      "Aperçu du dataset nettoyé:\n",
      "   TransactionID Produit  Quantité  Prix         Catégorie       Date\n",
      "0              1    pain       1.0  1.20       Boulangerie 2025-09-01\n",
      "1              2    lait       2.0  0.95           Laitage 2025-01-09\n",
      "2              3  beurre       1.0  2.80           Laitage 2025-09-01\n",
      "3              4  tomate       3.0  1.99  Fruits & Légumes 2025-09-02\n",
      "4              5  tomate       2.0  2.10  Fruits & Légumes 2025-02-09\n",
      "5              6   pates       1.0  0.89          Epicerie 2025-09-02\n",
      "6              7   pates       2.0  0.89          Epicerie 2025-02-09\n",
      "7              8     riz       1.0  1.10          Epicerie 2025-09-03\n",
      "8              9     riz       5.0  1.10          Epicerie 2025-09-03\n",
      "9             10  yaourt       6.0  0.45           Laitage 2025-09-03\n",
      "\n",
      "=== Informations finales ===\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 48 entries, 0 to 50\n",
      "Data columns (total 6 columns):\n",
      " #   Column         Non-Null Count  Dtype         \n",
      "---  ------         --------------  -----         \n",
      " 0   TransactionID  48 non-null     int64         \n",
      " 1   Produit        48 non-null     object        \n",
      " 2   Quantité       48 non-null     float64       \n",
      " 3   Prix           48 non-null     float64       \n",
      " 4   Catégorie      48 non-null     object        \n",
      " 5   Date           48 non-null     datetime64[ns]\n",
      "dtypes: datetime64[ns](1), float64(2), int64(1), object(2)\n",
      "memory usage: 2.6+ KB\n",
      "\n",
      "=== Valeurs manquantes finales ===\n",
      "TransactionID    0\n",
      "Produit          0\n",
      "Quantité         0\n",
      "Prix             0\n",
      "Catégorie        0\n",
      "Date             0\n",
      "dtype: int64\n",
      "\n",
      "=== Statistiques finales ===\n",
      "       TransactionID   Quantité       Prix                 Date\n",
      "count      48.000000  48.000000  48.000000                   48\n",
      "mean       24.625000   2.812500   2.009583  2025-08-08 00:00:00\n",
      "min         1.000000   1.000000   0.280000  2025-01-09 00:00:00\n",
      "25%        12.750000   1.000000   0.937500  2025-09-01 00:00:00\n",
      "50%        24.500000   2.000000   1.575000  2025-09-05 12:00:00\n",
      "75%        36.250000   3.250000   2.800000  2025-09-08 00:00:00\n",
      "max        50.000000  12.000000   7.500000  2025-12-09 00:00:00\n",
      "std        14.203124   2.606508   1.646926                  NaN\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Vérification finale ===\")\n",
    "print(f\"\\nDimensions du dataset final: {df.shape}\")\n",
    "print(f\"\\nAperçu du dataset nettoyé:\")\n",
    "print(df.head(10))\n",
    "\n",
    "print(f\"\\n=== Informations finales ===\")\n",
    "df.info()\n",
    "\n",
    "print(f\"\\n=== Valeurs manquantes finales ===\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "print(f\"\\n=== Statistiques finales ===\")\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fichier 'donnees_achats_propre.xlsx' exporté avec succès!\n",
      "Fichier 'donnees_achats_propre.csv' exporté avec succès!\n"
     ]
    }
   ],
   "source": [
    "# Export cleaned df\n",
    "df.to_excel('donnees_achats_propre.xlsx', index=False)\n",
    "print(\"\\nFichier 'donnees_achats_propre.xlsx' exporté avec succès!\")\n",
    "\n",
    "# Export in csv\n",
    "df.to_csv('donnees_achats_propre.csv', index=False, encoding='utf-8-sig')\n",
    "print(\"Fichier 'donnees_achats_propre.csv' exporté avec succès!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
